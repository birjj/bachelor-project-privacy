\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{mathabx}
\usepackage{mathrsfs}
\usepackage{tabularx}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{drawstack}
\usepackage{mdframed}
\usepackage{minted}
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber,style=alphabetic]{biblatex}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{cleveref}

\definecolor{codebackground}{RGB}{240, 240, 235}
\setminted{bgcolor=codebackground}

% support vertical lines in matrices
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
% angled fractions
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}
% text below matrices
\newcommand*{\putunder}[2]{%
  {\mathop{#1}_{\mathstrut #2}}%
}
% multiline comment
\newcommand{\comment}[1]{}
% vertical bars
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Bachelor Project (Differential Privacy) notes}

\author{Johan Ringmann Fagerberg (jofag17) - 1996-09-24}

\date{\today}

\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Non-technical primer \hfill\href{https://privacytools.seas.harvard.edu/files/privacytools/files/pedagogical-document-dp_0.pdf}{(PDF)}}

Traditional (\textit{statistical disclosure limitation}) techniques focus on de-identification through suppressing, aggregating and generalizing attributes of individuals in data. With the rise in computing power/analytical capabilities and amount of data, more and more cases of successful re-identification have happened, motivating the need for formal privacy guarantees.

Differential privacy gives this formal guarantee. Mostly studied in the context of collection, analysis and release of aggregate data. It is not a single tool, but a mathematical definition for quantifying privacy risks.

\subsection{Privacy guarantee}

Differential privacy aims to guarantee each participants privacy by ensuring that the information that can be gathered based on some dataset does not differ (appreciatively) based on whether or not said participants data is included or not. That is, the information about participant A that can be gathered from the dataset is (essentially) the same whether we're looking at the dataset, or the dataset with participant A's data excluded.

\begin{mdframed}
\textbf{Example of something differential privacy protects against:}\\
Professor A publishes article stating that 203 students at the university receive financial aid. A month later, professor B independently publish an article stating that 202 students at the university receive financial aid. Despite the information being taken from a de-identified dataset, some information can now be re-identified by correlating with a list of dropped out students during the timeframe.
\end{mdframed}

Differential privacy roughly works by adding noise to the data, such that you cannot discern between analysis of a dataset that includes, and one that doesn't include, any one specific individual.

Note that every analysis results in some leakage of information about the individuals being analyzed; for this reason there is a limit to how many analyses can be performed while providing an acceptable guarantee of privacy. For this reason, measuring privacy loss is critical when using differential privacy in practice.

Also note that there are cases where we can still learn something about participant A, even if their data isn't included in the dataset - differential privacy does not protect against this.

\begin{mdframed}
\textbf{Example of something differential privacy does not protect against:}\\
Person A knows that person B is a school teacher with 5 years of experience, who will soon start a job in a new district. Person A stumbles upon said district's teacher union's average salary figures. They can now accurately estimate person B's new salary. Since the dataset wasn't based on person B's data, differential privacy's guarantees do not help here.
\end{mdframed}

\subsection{Privacy loss parameter}

Differential privacy only requires that the output of an analysis is \textit{approximately} the same, whether any one individuals data is included or not (otherwise we'd be left with no individuals to base the data on). A parameter $\epsilon$, called the privacy loss parameter, quantifies how big this deviation can be. Small values of $\epsilon$ gives good privacy, but low accuracy.

Formally, if $E$ is some event in the analysis of the dataset, and $E^*$ is the same event when the dataset has participant A's data excluded, then

$$\Pr(E) \leq (1 + \epsilon) \cdot \Pr(E^*) \quad \text{and} \quad \Pr(E^*) \leq (1 + \epsilon) \cdot \Pr(E)$$

This privacy loss parameter is used when calculating how much noise to perturb the dataset with. A simple possibility (when dealing with cumulative numeric data, e.g. the number of HIV-positive individuals in a sample) is the pick the noise $Y$ from a normal distribution with zero mean and a standard deviation $1/\epsilon$ (proof that $m' = m+Y$ preserves differential privacy requires slightly different choice, but that's beyond scope of non-technical primer).

\subsection{Types of analyses under differential privacy}

Differentially private algorithms for different kinds of analyses have been published. This includes (but is not limited to)

\begin{description}
    \item[Count queries] i.e. the number of individual records in the data satisfying some predicate. Made differentially private through addition of random noise.
    
    \item[Histograms] Classification of data points into disjoint categories. Made differentially private through added noise for each category.
    
    \item[Cumulative distribution function] When talking about ordered domains of integers or real numbers, a CDF is a function that, for some value $x$, describes the number of data points $< x$ (e.g. number of people below the age of 65). Can be used to calculate median, among other stats.
    
    \item[Linear regression] Often used in statistical sciences to fit a presumed model to a dataset. Although the differentially private versions may sometimes hide existing correlations, research is ongoing in minimizing and controlling this effect.
    
    \item[Clustering] Groups data points such that points in the same cluster are more similar to each other than to points in other clusters. Often used as an exploratory tool, to gain insight into data's sub-classes.
    
    \item[Classification] Classification of data points into a set of categories based on machine learning (e.g. a pre-trained classifier, trained on known data). Differentially private classification algorithms exist for many classification tasks, with (theoretically) similar performance to their non-private counterparts.
\end{description}

\subsection{Practical challenges of differential privacy}

\begin{description}
    \item[Accuracy] Because of the added noise to preserve privacy, differentially private computations are less accurate than their non-private counterparts. This increases the minimal sample size required to produce meaningful statistics. As a rule of thumb, almost no utility is expected from a dataset containing $1/\epsilon$ or fewer records (without amplification, e.g. "secrecy of the sample" - beyond the scope of primer).
    
    For some types of computations, methods of calculating expected accuracy have been developed, given an $\epsilon$ value (or the inverse, providing a desired accuracy and outputting an $\epsilon$ value).
    
    \item[The "privacy budget"] Another way to think of $\epsilon$ is as a privacy budget. If a single analysis is to be performed on a dataset, then it may use up the entire budget (i.e. use $\epsilon$ directly as its privacy loss parameter). If we want to do more than one computation on the dataset, then we need a way to combine privacy losses.
    
    A number of composition theorems have been developed. In particular, they state that the composition of two differentially private analyses results in a privacy loss that is bounded by the sum of the privacy loss of each of the analyses. That is, if the privacy budget is $\epsilon = 0.1$, then an analyst who wishes to run two computations on the dataset may choose to set $\epsilon_1 = 0.02$ and $\epsilon_2 = 0.05$, for a bounded privacy loss of $0.07$ - still within the privacy budget. Better bounds \textit{may} be obtained using more sophisticated approaches.
\end{description}

\subsection{Additional topics}

\begin{description}
    \item[Property of the computation] Unlike most previous anonymization techniques (e.g. $k$-anonymity), differential privacy is not a property of the outputted data. It is instead a property of the computation that produces the output.
    
    \item[Group privacy] Although differential privacy uses an individual's privacy as baseline, we are often interested in cases where a group of people share some data. It can be shown that for a group of $k$ individuals, the difference between a differentially private computation on the dataset, and on the dataset with the group's data removed, is at most $k\cdot\epsilon$. Meaningful privacy guarantees can be provided to groups of up to size $k \approx 1/\epsilon.$
    
    \item[Amplification - secrecy of the sample] In cases where the data comes from a uniformly random sample (i.e. the individuals are chosen uniformly randomly from some larger set, and are chosen independently of eachother), and the sample is kept confidential, we are able to apply a theorem known as "secrecy of the sample". This allows for savings in the privacy loss paramter $\epsilon$ corresponding to the ratio of the sizes between the dataset and the larger population.
    
    For example, for a uniformly random sample of 3000 people from the city of Boston (population 600 000), the savings can be $3000/600000 = 0.05$ - meaning that greater accuracy, corresponding to a 0.05 decrease in $\epsilon$ can be provided by a differentially private analysis.
    
    Unfortunately, sampling is usually not uniformly random. This can be counterbalanced by being conservative with the population estimate. Keeping samples confidential also sometimes requires extra care (e.g. going to their house for interviews means neighbors may now know) - and also requires that who has \textit{not} been sampled is kept confidential.
\end{description}

\section{"Privacy book" (chapter 1-3) \hfill \href{https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf}{(PDF)}}

\subsection{Chapter 1. The promise of differential privacy}

Differential privacy is a definition of privacy tailored to the problem of privacy-preserving data analysis, tackling the paradox of learning something about a population while revealing nothing about an individual (\textit{nearly} nothing, to avoid the whole paradox thing).

It protects against linkage attacks (e.g. anonymized Netflix dataset where individuals were identified by cross-referencing with IMDb), amongst other things.

\subsection{Chapter 2. Basic terms}

Unlike e.g. encryption, it isn't reasonable to require that an adversary can learn nothing; the intended recipient and the adversary is one and the same, so if the adversary can learn nothing then the dataset is useless. \bigskip

For the formal definition we define $\mathbb{N}^{|\mathcal{X}|}$ as the histogram of our database (an entry for each possible type in our universe $\mathcal{X}$, with the entry's value being the number of entries of that type that exists in our database), and the $\ell_1$ norm as $||x-y||_1 = \sum_{i=1}^{|\mathcal{X}|} |x_i - y_i|$. We can then define "differential privacy" as:

\begin{mdframed}
    A randomized algorithm $\mathcal{M}$ with domain $\mathbb{N}^{|\mathcal{X}|}$ is $(\epsilon, \delta)$-differentially private if for all $\mathcal{S} \subseteq \text{Range}(\mathcal{M})$ and for all $x,y \in \mathbb{N}^{|\mathcal{X}|}$ such that $||x-y||_1 \leq 1$:
    
    $$\Pr(\mathcal{M}(x) \in \mathcal{S}) \leq \exp(\epsilon) \Pr(\mathcal{M}(y) \in \mathcal{S}) + \delta,$$
    
    where the probability space is over the coin flips of the mechanism $\mathcal{M}$. If $\delta=0$, we say that $\mathcal{M}$ is $\epsilon$-differentially private.
\end{mdframed}

That is, for any two possible databases $x,y$, which differ in at most one entry, the probability for the output $\mathcal{M}(x)$ is no larger than the given linear function of the probability for the output $\mathcal{M}(y)$. Since $x,y$ are interchangeable, this essentially guarantees that we cannot tell whether a given entry was in the database or not with better than a bounded certainty, if we're only given the output of the algorithm. \bigskip

We also introduce the measure of \textit{privacy loss} incurred by observing some output $\xi$, defined as

$$\mathcal{L}_{\mathcal{M}(x)||\mathcal{M}(y)}^{(\xi)} = \ln \left( \frac{\Pr(\mathcal{M}(x) = \xi)}{\Pr(\mathcal{M}(y) = \xi)} \right).$$

This can be positive (when it's more likely to observe $\xi$ under $x$ than $y$) or negative (the inverse). It can be shown (lemma 3.17 in the book) that for any adjacent databases the absolute value of this privacy loss can be bounded by $\epsilon$ with probability at least $1-\delta$.

\bigskip

Differential privacy is safe from postprocessing. That is, the composition of any function with a $(\epsilon,\delta)$-differentially private mechanism is itself $(\epsilon,\delta)$-differentially private.

Differentially private algorithms can composition nicely. The composition of $k$ $(\epsilon_i, \delta_i)$-differentially private mechanisms results in a $(\sum_i \epsilon_i, \sum_i \delta_i)$-differentially private mechanism. More on that in chapter 3.

$\epsilon$-differentially private mechanisms also generalize nicely for databases that differ in more than one entry. For databases that differ in $k$ entries (a "group size" of $k$), we get a $(k\epsilon)$-differentially private mechanism.

For $(\epsilon, \delta)$-differentially private mechanisms this is much worse, resulting in a $(k\epsilon, k\delta\cdot\exp[(k-1)\epsilon])$-differentially private mechanism. 

\bigskip

To recapitulate, differential privacy has several qualitative properties:

\begin{description}
    \item[Protection against arbitrary risk] It protects against any risk involved with participating in a dataset. This goes beyond e.g. protection against re-identification.
    
    \item[Automatic neutralization of linkage attacks] Including those against datasets that do not yet exist.
    
    \item[Quantification of privacy loss] It is not a binary concept, and we have a way to measure how much privacy loss. This makes it possible to compare different techniques.
    
    \item[Composition] We analyze cumulative privacy losses over multiple computations. This enables complex differentially private algorithms to be designed from simpler differentially private building blocks.
    
    \item[Group privacy] We can analyze privacy loss incurred by groups, e.g. families in health studies.
    
    \item[Closure under post-processing] No post-processing of the output of a differentially private algorithm is ever able to harm the privacy provide, regardless of the auxiliary information available.
\end{description}

\subsection{Chapter 3. Basic techniques and composition theorems}

\textcolor{red}{TODO}

\end{document}