\documentclass[12pt]{article}

\include{includes/head}
\addbibresource{main.bib}

% custom commands
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\comment}[1]{}
% shorthand command aliases
\newcommand{\fancy}{\mathcal}
% command redefinitions
\renewcommand{\epsilon}{\varepsilon}

\begin{document}

\include{includes/titlepage}

\renewcommand{\abstractname}{Abstract}
\begin{abstract}
\todo{Todo}
\end{abstract}

\begin{center} \bf Keywords \end{center}

\thispagestyle{empty}
\tableofcontents
\newpage

\section{Introduction}

Privacy-preserving data collection has long been an area of interest for many but never before has it seen as significant focus as it has over the last few decades. With the global shift towards connected lifestyles that comes with the advent of the Internet, data can now be gathered from almost every aspect of our lives---and with that comes a need to guarantee subject privacy.

For a long time, this has been attempted primarily through techniques that ``anonymizes'' incoming data, attempting to remove any information that may lead to privacy loss. Unfortunately, this approach has shown itself to be fallible: without formal guarantees, it relies on the foresight of the data analysts who decide what data to remove, which often leads to de-anonymization when auxiliary data---or simply insufficient anonymization---is present.

\bigskip

Historically it has been particularly challenging to provide formal guarantees in the ``non-interactive'' setting\footnote{As opposed to the ``interactive'' setting, where the data collector keeps the data set private, but allows users to query the data in some limited way.}, where data sets are published publicly after some form of anonymization. In this setting, the primary defense has been the removal of known identifiers, such as names and birthdates, or other limitation of information from the data set through techniques like sub-sampling. These approaches have seen widespread use, including in particularly sensitive areas like health data, but have repeatedly fallen under re-identification attacks (see e.g. \cite{reidentification2011}).

In recent years, an alternative approach has emerged in the form of \emph{differential privacy}. The culmination of a line of research explored in \cite{precursor_2003,precursor_2004,precusor_2005}, Dwork et al. formalize the concept in \cite{dworketal2006} by introducing their formal description of ``privacy loss''. Differential privacy not only offers a way to describe and compare differentially private algorithms but also guarantees that the privacy provided by such algorithms does not falter in the presence of unforeseen auxiliary information or attack approaches.

Differential privacy has since been subject to considerable interest. It has seen numerous high-profile applications, including the 2020 US census \cite{us_census}, as well as data gathering from companies like Apple \cite{apple_differential,apple_differential_loss}, Google \cite{google_rappor,google_prochlo} and Microsoft \cite{dworketal2006,microsoft_telemetry}. Even beyond its roots in cryptography and statistics, it has also found application in areas like machine learning \cite{ml_abadi,ml_shokri,ml_papernot}. With the growing focus on digital data collection and the privacy concerns that follow, differential privacy has become an important tool for academics and companies alike.

\bigskip

In this thesis, we will be exploring the definition of differential privacy. In section \ref{sec:diffpriv} we will explore its origins and definition. In section \ref{sec:composition} we will show some key theorems of differential privacy that allow us to describe how the privacy provided by algorithms is affected as they are composed. In section \ref{sec:achieving} we will describe some general mechanisms for designing private algorithms. In section \ref{sec:variants} we will describe common variants of differential privacy. Finally, we will be exploring one particular algorithm for differentially private histogram estimation in the local setting \cite{microsoft_telemetry}, and evaluate its performance in low-population scenarios.

\section{Differential privacy \label{sec:diffpriv}}

\subsection{Motivation \label{sec:motivation}}

Before exploring the concept of differential privacy, it is worthwhile to spend some time understanding the problem that it aims to solve. \bigskip

As data scientists, we often want to guarantee subjects (meaning the people whose responses our database is built from) that no ``private data'' can be leaked. This is especially important when working with sensitive data, like medical histories or crime statistics, where the consequences of privacy breaches are significant.

Giving such a guarantee naturally requires defining what it means to reveal ``private data''. Intuitively we have to balance our interest in interesting research with our interest in protecting subject privacy: we cannot reveal \emph{nothing}, as that would make it impossible to conclude anything worthwhile, but we also cannot reveal too much, as that would violate our promise of privacy. The question then is: what can (and can't) we reveal?

The common distinction is made between data that enables identification of the subjects, and data that doesn't. This builds on the intuition that a subject's privacy specifically relates to knowing something about \emph{them}, and cannot be harmed without some link between them and the data.

Consider the example of a smoker who participated in a study related to lung cancer: if the database of the study is released unmodified, it will be trivial for anyone to determine whether they had lung cancer, which would certainly be considered a breach of privacy. If the database is modified in such a way that no participant can be identified, it might be possible to determine that \emph{someone} had lung cancer, but not who it was. In order for the database to be useful, it must also be possible to draw interesting conclusions (such as a link between smoking and lung cancer) from the database, despite the anonymization efforts. It is interesting to note that this enables people who already know that the subject is a smoker to learn something new about them (such as that they have an increased risk of lung cancer), but that we do not consider this a breach of privacy---in fact this could be learned even if the subject \emph{hadn't} participated in the study. \bigskip

Historically the privacy guarantees given by various anonymization techniques have been much more limited than the one described above, protecting primarily against specific and limited attacks. It has been particularly difficult to protect subject privacy in the so-called ``non-interactive'' setting, where data sets are published publicly after some form of anonymization. Unlike the interactive setting, where the data controller is the one responding to queries, and therefore is able to make decisions based on the specific queries made, this setting requires strong generalized anonymization. The primary defense has long been the removal of known identifiers, such as names and birthdates, or other limitation of information from the data set through techniques like sub-sampling.

The problem with these defenses is that they are often vulnerable to unforeseen auxiliary data, and they cannot be generalized across different projects. Although the removal of birthdates and names may be sufficient for one use case, that doesn't mean they are generally sufficient: in fact there have been numerous real-world examples where data scientists have released what they believed to be ``anonymized'' data sets, ranging from medical records to social networking data, just to have them be successfully de-anonymized by researchers (see e.g. \cite{reidentification2011}).

Consider the previous example of a smoker participating in a study related to lung cancer. The data collector may argue that by aggregating the data by age groups, such that the only information released is how many subjects in each age group tested positive for lung cancer, each individual subject are protected against identification. Nonetheless, a neighbor may know that the subject is participating in the study, be able to observe on which days they leave for the hospital, and know that the study releases updated statistics frequently. Armed with this auxiliary information, even the previously innocuous-seeming aggregate information may be enough to make an educated guess, or even identify, whether the subject in question has been diagnosed with lung cancer by simply observing changes in the expected age group. \bigskip

An alternative approach that has regained popularity in the last 20 years is based on perturbing the data by introducing noise.

A classic example is that of randomized response, where subjects are asked to answer randomly to some degree: prior to answering some sensitive yes/no question they are asked to flip a coin. If the coin lands on heads, they should answer truthfully. If not, they should answer ``yes''\footnote{Or ``no'' if that is the sensitive option. In some variants they are asked to flip a second coin and answer ``yes'' if it lands on heads, ``no'' otherwise.}. In this way subjects have a degree of \emph{plausible deniability}---even if they answered ``yes'' to a sensitive question, they can later claim that that coin simply landed on tails. The data collector is nonetheless able to calculate the expected proportion of respondents with each true answer using simple mathematics (although with significant uncertainty for small survey sizes).

This intuition can be extended further, and combined with other approaches. Take the example from earlier, where a neighbor was able to guess whether a subject had lung cancer by observing aggregate statistics. Had the data collector added noise to those statistics, the neighbor would have no way of knowing whether an increase in the number of subjects with lung cancer was a result of an actual diagnosis, or merely the result of the noise---assuming the noise was correctly applied. \bigskip

With so many different ways to implement privacy, a natural question is ``how---and how much---should we perturb our data in order to protect subject privacy adequately''. It is this question that differential privacy aims to provide a way to answer.

\subsection{Differential privacy \label{sec:promise}}

Differential privacy was formalized in 2006 by Dwork et al. \cite{dworketal2006}, following a line of research into privacy provided by noise-perturbation of data sets \cite{precursor_2003,precursor_2004,precusor_2005}. It tackles the problem of how we can mathematically guarantee that the privacy of a subject cannot be breached by the output of some algorithm (i.e. a process that takes our data set and provides an output)---and how we can design algorithms that provide such guarantees.

The notion of privacy introduced in the paper relies on the intuition that an algorithm cannot violate the privacy of someone whose data isn't used in it. If the output of the algorithm didn't depend at all on a particular subjects data, then the output cannot breach said subjects privacy. If we could provide this guarantee to all of our subjects, then the algorithm wouldn't be a privacy risk at all.

This guarantee is unfortunately impossible to provide to all subjects. As proven by Dwork in the follow-up paper \cite{dwork2006_diffpriv}, revealing nothing about any subject while still providing utility is impossible. If we were to give this guarantee to each of our subjects, then we could not learn anything from our data set, rendering it useless. Instead, we can promise our subjects that the influence of their data on the output is \emph{almost} none---that the likelihood of seeing each possible output is almost the same with or without their data. This guarantee still gives them a strong degree of privacy through plausible deniability, depending on our definition of ``almost''. \bigskip

Consider the example from earlier, with a subject participating in a study related to lung cancer. An adversary may wish to use our output to learn something about the subject, in breach of the subjects privacy. With our guarantees it is very limited what they can learn: even if they see an output that indicates that the subject had lung cancer, they are almost as likely to see that output if the subject didn't have lung cancer. No matter the output, the certainty with which they can come to a conclusion one way or the other is limited by our guarantees.

It's important to note that this doesn't mean that the adversary can't learn \emph{anything} about our subject. If they're interested in information that doesn't depend on the subjects specific response---e.g. whether the subjects age group has a heightened risk of lung cancer---then our privacy guarantee does not protect this information. We only consider it a privacy breach if they can learn something that they wouldn't have been able to learn if the subject hadn't participated in the study. \bigskip


This leads us to our definition of differential privacy, here formulated as done by Dwork in \cite{dwork2006_diffpriv}:

\begin{mdframed}
    \textbf{Differential privacy:} Given an algorithm $\fancy{K}$, we say that it provides $\epsilon$-differential privacy if for any two data sets $D_1$ and $D_2$ differing on at most one element, and any $S \subseteq \text{Range}(\fancy{K})$, it holds that
    \begin{equation}\label{eq:diffpriv}
        \Pr[\fancy{K}(D_1) \in S] \leq \exp(\epsilon) \cdot \Pr[\fancy{K}(D_2) \in S]
    \end{equation}
\end{mdframed}

For some use cases it may be more useful to state this as the equivalent
\begin{equation*}
    \frac{\Pr[\fancy{K}(D_1) \in S]}{\Pr[\fancy{K}(D_2) \in S]} \leq \exp(\epsilon).
\end{equation*}\bigskip

This definition gives us a formal measure for just how private an algorithm is. The $\epsilon$ parameter describes just how close two probabilities have to be for us to consider them ``almost'' unchanged. This means that we are able to compare algorithms, even across problem statements, on how well they protect subject privacy. We are able to describe the effect that changes, such as composition with other algorithms, have on the privacy guarantees provided by an algorithm---in many cases this can even be done generally, giving us composition theorems that hold for any $\epsilon$-differentially private algorithm.

Having such formal measures further allows us to communicate the guarantees given by our algorithms in concrete terms. Going from vague best effort guarantees to concrete and formally proven privacy protection is a big win for subject privacy. It allows subjects to compare and evaluate surveys based on a simple measurement; this is especially important in a world where privacy is becoming a bigger and bigger topic in the public discourse, while systems processing data are becoming more and more complex. Having formal guarantees also helps the legislative enforcement of privacy-preservation that is becoming prevalent in some parts of the world, a process that has historically been difficult and vague as a result of poor formal definitions. \bigskip

An interesting effect of this privacy definition is that a differentially private algorithm may not have any outputs that are only possible if a subject has given a particular answer. Each output in the range of the algorithm \emph{must} have a non-zero probability, no matter the true data set. This hints at the common implementation technique of perturbing either the data or output with noise, which we will explore more in section \ref{sec:achieving}.

\subsubsection{The privacy parameter $\epsilon$ \label{sec:epsilon}}

At the core of our definition is the $\epsilon$ parameter. This describes how much a single entry can affect the output probabilities for us to consider them ``almost'' unchanged, with high values permitting high privacy loss, and low values protecting subject privacy well. It is often known as the ``privacy measure'' or ``privacy parameter'', as it allows us to directly compare the privacy guarantees provided by two differentially private algorithms.

Which values we consider reasonable for $\epsilon$ depends on the specific usage scenario in question. For particularly sensitive data, where we want third parties to be unable to determine a subjects response with any reasonable certainty, we may want $\epsilon$ values less than 1. For less sensitive data we may find higher values, sometimes as high as 10, acceptable. It is important to note that as $\epsilon$ is used as an exponent in definition \ref{eq:diffpriv}, the privacy guarantees quickly degrade as $\epsilon$ increases---a 10-differentially privacy algorithm allows for an output to be 22000 times more likely given a specific subjects response, while a 2-differentially private algorithm only allows a factor of 7.

The privacy parameter $\epsilon$ is also often called the ``privacy budget''. As we will see in section \ref{sec:composition}, combinations of differentially private algorithms often lead to differentially private algorithms whose $\epsilon$ depends on the $\epsilon$ of the combined algorithms. If we define a global $\epsilon$ value, describing the privacy loss we are willing to accept for our subjects, then we are able to pick algorithms that each ``use'' a part of this privacy budget. This notion of $\epsilon$ as a budget for algorithms to be limited allows us to design our algorithms with the privacy guarantees in mind from the very beginning.

It is often the case for differentially private algorithms that low values of $\epsilon$, which provide good privacy protection, also degrades the utility of the output. If the algorithm masks the contribution of subjects using noise, then more noise is required as $\epsilon$ decreases, leading to less accurate results. For this reason there is often an inherent balancing act when it comes to differential privacy: low values of $\epsilon$ provide good privacy, but poor data utility, while high values of $\epsilon$ provides the inverse. As such, finding an $\epsilon$ value that is acceptable both in terms of privacy and data utility is crucial. Common values for large commercial data sets \cite{apple_differential,us_census} are in the range of 4-6, but since commercial data collectors are often incentivized to prioritize utility over subject privacy, it is important that each subject is able to evaluate the $\epsilon$ values of a given study and decide for themselves whether they are satisfactory. \bigskip

It is important to note that, although $\epsilon$ allows us to compare the privacy guarantees offered by two algorithms directly, we must be careful in the conclusions we draw. In particular, we are not able to compare algorithms for simply failing to reach a certain degree of differential privacy: an algorithm that is not $\epsilon$-differentially private for some $\epsilon$ may simply be $2\epsilon$-differentially private, which is almost identical for low values of $\epsilon$. Another algorithm failing to achieve the same $\epsilon$-differential privacy may release the data set unaltered.



\subsection{Attributes and composition theorems \label{sec:composition}}

One of the great aspects of our definition of differential privacy is that we can formally prove attributes for differentially private algorithms, and how their privacy guarantees are affected by changes such as composition---and that we are able to do this generally, and not just for one specific algorithm.

\subsubsection{Immunity to post-processing}

One such attribute that we are able to formally prove is that of \emph{immunity to post-processing}. Recall our example from section \ref{sec:motivation}, where a neighbor was able to make an educated guess at whether a subject had lung cancer based on auxiliary data. This was an informal example of post-processing: the adversary was able to combine auxiliary data with the survey output, and process it into a data set that violated the subject's privacy to a higher degree than the survey output on its own did. It is fairly simple to show that post-processing leading to privacy loss is \emph{not} possible for the output of differentially private algorithms (assuming the true data set is kept secret), as done in \cite{dwork_privacybook}:

Let us imagine an $\epsilon$-differentially private algorithm $\fancy{K} : \fancy{D} \to \fancy{R}$. For any post-processing function $f : \fancy{R} \to \fancy{R}'$, we can show that the algorithm $f \circ \fancy{K}$ is also $\epsilon$-differentially private---that is, there are no post-processing functions that increase the privacy loss of the original algorithm. For any two neighboring data sets $D_1$ and $D_2$, and any event $S \subseteq \fancy{R}'$, we define $T = \{r|f(r) \in S\}$ to be the outputs of $\fancy{K}$ that lead to event $S$. Then we have
\begin{align*}
    \Pr[\fancy{K}(D_1) \in T] &= \Pr[f(\fancy{K}(D_1)) \in S]  \\
    &\leq \\
    \exp(\epsilon)\cdot\Pr[\fancy{K}(D_2) \in T] &= \exp(\epsilon)\cdot\Pr[f(\fancy{K}(D_2)) \in S],
\end{align*}
where the inequality follows from definition \ref{eq:diffpriv}. This shows that $f \circ \fancy{K}$ is also $\epsilon$-differentially private, if $f$ is deterministic. It can be extended to non-deterministic functions \todo{as these can be decomposed into a convex combination of deterministic functions, and convex combinations preserve differential privacy}.

It is important to note that this proof fails if $f$ has knowledge of the original data set. In this case the equalities do not hold, meaning the result is not necessarily $\epsilon$-differentially private. The immunity to post-processing therefore only holds when the input to $\fancy{K}$ is kept private. \bigskip

This proof formalizes an important aspect of our privacy guarantee. With this immunity we know that our guarantee cannot be compromised or harmed after we release our data, by anyone who does not have access to our original data set---unlike many other anonymization techniques that can often be de-anonymized later as unexpected auxiliary data becomes available. If the data set is properly anonymized using differential privacy, and the original data set is discarded, then our privacy guarantees will hold perpetually, regardless of new techniques or auxiliary data being discovered.

\subsubsection{Group privacy}

Another important attribute of differentially private algorithms is that the privacy guarantee readily extends to groups of subjects. There are cases where we may wish to describe how our guarantee holds for groups of subjects, e.g. families, so as to ascertain how much privacy loss the group as a whole can suffer.

Formally this would mean describing how our probabilities change for any two data sets $D_0$ and $D_k$ that differ in at most $k$ entries, where $k$ is the size of the group we are interested in. By constructing data sets $D_0, D_1, \dots, D_k$ such that data sets $D_i,D_j$ differ in at most $|i-j|$ entries, we can see from definition \ref{eq:diffpriv} that
\begin{align*}
    \Pr[\fancy{K}(D_0) \in S] &\leq \exp(\epsilon) \cdot \Pr[\fancy{K}(D_1) \in S] \\
        &\leq \exp(\epsilon) \cdot \exp(\epsilon) \cdot \Pr[\fancy{K}(D_2) \in S] \\
        &\qquad\vdots \\
        &\leq \exp(\epsilon)^k \cdot \Pr[\fancy{K}(D_k) \in S] = \exp(k\epsilon) \cdot \Pr[\fancy{K}(D_k) \in S].
\end{align*}

This shows that any $\epsilon$-differentially private algorithm is $k\epsilon$-differentially private for groups of size $k$. \bigskip

Note that this means that the $\epsilon$ value for the group algorithm is linearly dependent on the size of the group. As $\epsilon$ is an exponent in definition \ref{eq:diffpriv}, this means that our privacy guarantees quickly degrade as $k$ increases, requiring the same considerations as those described in section \ref{sec:epsilon}.

\subsubsection{Linear combinations of differentially private algorithms}

There are cases where we may wish to employ two differentially private algorithms on the same data set, and release both outputs. This scenario can be modelled as the combined algorithm $\fancy{K}_{1,2}(x)=\left(\fancy{K}_1(x), \fancy{K}_2(x)\right)$, where $\fancy{K}_1$ is $\epsilon_1$-differentially private, and $\fancy{K}_2$ is $\epsilon_2$-differentially private. It can be shown that $\fancy{K}_{1,2}$ is then $\epsilon_1+\epsilon_2$-differentially private.

The proof of this is again rather simple. For any two data sets $D_1,D_2$ differing in at most one entry, and any $(r_1,r_2) \in \text{Range}(\fancy{K}_{1,2})$, we have
\begin{align*}
    \Pr[\fancy{K}_{1,2}(D_1)=(r_1,r_2)] &= \Pr[\fancy{K}_1(D_1)=r_1]\cdot\Pr[\fancy{K}_2(D_1)=r_2] \\
        &\leq \exp(\epsilon_1) \Pr[\fancy{K}_1(D_2)=r_1] \cdot \exp(\epsilon_2) \Pr[\fancy{K}_2(D_2)=r_2] \\
        &= \exp(\epsilon_1 + \epsilon_2) \cdot \Pr[\fancy{K}_1(D_2)=r_1] \cdot \Pr[\fancy{K}_2(D_2)=r_2] \\
        &= \exp(\epsilon_1 + \epsilon_2) \cdot \Pr[\fancy{K}_{1,2}(D_2)=(r_1,r_2)],
\end{align*}
showing that $\fancy{K}_{1,2}$ is $\epsilon_1+\epsilon_2$-differentially private.

This proof naturally extends to any number of combined algorithms. The combined algorithm $\fancy{K}_{1,2,3}(x)=\left(\fancy{K}_1(x), \fancy{K}_2(x), \fancy{K}_3(x) \right)$ can be modelled as $\fancy{K}_{1,2,3}(x)=\left(\fancy{K}_{1,2}(x), \fancy{K}_3(x)\right)$, and is thus $\epsilon_1 + \epsilon_2 + \epsilon_3$-differentially private---and this can be extended indefinitely. The algorithm $\fancy{K}_{[k]}(x)=\left(\fancy{K}_1(x), \dots, \fancy{K}_k(x) \right)$ is therefore $\sum_{i=1}^k \epsilon_i$-differentially private, if each $\fancy{K}_i$ is $\epsilon_i$-differentially private. \bigskip

This result is valuable to us. It tells us that we can combine differentially private algorithms without any further work or considerations. Simply releasing the outputs of each algorithm is in itself differentially private, and we can describe precisely how much privacy loss this incurs.

This is also an example of a composition theorem that allows us to practically apply the ``privacy budget'' philosophy as described in section \ref{sec:epsilon}. If the total work we perform on a data set is described by a set of $n$ differentially private algorithms, where we release the output of each, then this combination theorem tells us how high the $\epsilon$ value may be for each algorithm before we exceed our budget. \bigskip

This combination theorem is the only composition theorem we will show for $\epsilon$-differential privacy. More advanced composition theorems exist for the differential privacy variant known as $(\epsilon,\delta)$-differential privacy (see section \ref{sec:variant_eps_delta}), but are beyond the scope of this thesis. Interested readers may instead refer to \cite[ch.~3]{dwork_privacybook}.

\subsection{Achieving differential privacy \label{sec:achieving}}

We have seen some of the useful proofs that give differential privacy its utility, but have yet to see an example of a differentially private algorithm. Our definition \ref{eq:diffpriv} does not specify anything about how the algorithm is constructed, only about the way it acts---but as we saw in section \ref{sec:diffpriv}, it does hint at one common implementation technique: noise perturbation.

This consists of adding noise to either the input or the output of the algorithm, so as to lessen the impact a single subjects data can have on the perturbed output. This noise has to be crafted with care, so as to ensure that the output lives up to the definition of differential privacy. In this section we will explore two common methods for designing such noisy algorithms, and show that they achieve differential privacy. \bigskip

Before we do that, we will introduce a useful measure known as the $L_1$ sensitivity of an algorithm. This measures the biggest difference a single entry can cause in the output of the algorithm. Formally, we say that the $L_1$ sensitivity of an algorithm $f$ is the smallest number $\Delta f$ such that for any data sets $D_1,D_2$ differing in exactly one entry,
\begin{equation}\label{eq:L1_sensitivity}
    ||f(D_1)-f(D_2)||_1 \leq \Delta f.
\end{equation}

This measure is useful to us, as it intuitively measures how much noise we might have to add to mask the contribution of a single subject. For example, if the algorithm $f$ is a sum over the data set with each entry consisting of either the value $0$ or $1$, then we wouldn't have to mask a contribution of more than $\Delta f = 1$. For other algorithms, such as averaging over a data set with each entry being yearly wages in dollars, we have significantly higher (and potentially unbounded) $L_1$ sensitivity.

As we will see we can use this $L_1$ sensitivity measure to scale the amount of noise to add to the algorithm to achieve a specific level of $\epsilon$-differential privacy.

\subsubsection{The Laplace mechanism}

One probability distribution that lends itself nicely to achieving differential privacy is the Laplace distribution $\text{Lap}(b)$. This distribution is centered on 0, has a standard deviation $b$, and a density function
\begin{equation*}
    h(x)\propto \exp\left(\frac{-|x|}{b}\right).
\end{equation*}
We will show that by drawing noise from this distribution and using it to perturb the output of a numeric algorithm $f$, we can achieve $\epsilon$-differential privacy---a process known as the Laplace mechanism \cite{dworketal2006}. \bigskip

First, note that for any inputs $x,x'$ to this density function we have
\begin{align*}
    \frac{h(x)}{h(x')} &= \exp\left(\frac{-|x|}{b}-\frac{-|x'|}{b}\right) = \exp\left(\frac{|x'|-|x|}{b} \right) \\
        &\leq \exp\left( \frac{|x'-x|}{b} \right) = \exp\left( \frac{|x-x'|}{b} \right)
\end{align*}
where the inequality comes from the triangle inequality.

This means that if we define the perturbation of some numeric algorithm $f: \fancy{D} \to \mathbb{R}$ to be $f'(x) = f(x)+Y$ where $Y$ is a random variable drawn from $\text{Lap}(b)$, then we can combine this with definition \ref{eq:L1_sensitivity} when looking at two neighboring data sets $D_1,D_2$ to get
\begin{align*}
    \frac{\Pr[f'(D_1)=k]}{\Pr[f'(D_2)=k]} &= \frac{h(k - f(D_1))}{h(k - f(D_2))} \\
        &\leq \exp\left( \frac{|f(D_1)-f(D_2)|}{b}\right) \\
        &\leq \exp\left(\frac{\Delta f}{b}\right).
\end{align*}
From this we can see that if we draw $Y$ from the distribution $\text{Lap}(\Delta f/b)$ then $f'(x)$ becomes $\epsilon$-differentially private.

This generalizes nicely to higher dimensions, meaning the perturbation of an algorithm $f : \fancy{D} \to \mathbb{R}^d$ defined as $f'(x) = f(x) + (Y_1,\dots,Y_d)$ is $\epsilon$-differentially private when $Y_i$ are independent random variables drawn from $\text{Lap}(\Delta f / \epsilon)$. \bigskip

This result shows an easy way to achieve differential privacy for numeric algorithms by adding noise proportional to the sensitivity of the algorithm. This is particularly interesting for algorithms with low sensitivity. Notable examples of this includes counting queries (i.e. how many elements satisfy a given property) which have $\Delta f=1$ and disjoint partitioning (e.g. histograms) which have $\Delta f=2$. Other algorithms which have high sensitivity would require a lot of noise, worsening their utility. Alternate mechanisms exist that may be able to provide differential privacy with greater utility. Examples of this include smooth sensitivity \cite{nissim_smoothsens} which scales the noise not to the global sensitivity, but to the local sensitivity around each entry, as well as problem-specific algorithms that may not follow a generalized approach like the Laplace mechanism, but still provide differential privacy.

\subsubsection{The exponential mechanism}

While the Laplace mechanism is a useful and general approach to providing differential privacy for numeric functions. However, it is not always desirable to perturb the output of the algorithm, even if those perturbations may be small. Furthermore, there are many use cases where the algorithms we want to be private \emph{aren't} numeric, but have arbitrary ranges.

The exponential mechanism \cite{sherry_exponentialmech} takes a slightly different approach, here described using notation from \cite{dwork_privacybook}. For an arbitrary algorithm $f : \fancy{D} \to \fancy{R}$ it introduces the concept of a utility measure $u : \fancy{D} \times \fancy{R} \to \mathbb{R}$. This utility measure gives a numeric value to any pair $(d,r)$ where $d \in \fancy{D}, r \in \fancy{R}$. The intuitive goal for the private algorithm $f'(d)$ then is to pick the value $r \in \fancy{D}$ for which $u(d,r)$ is maximized, and do so in a way that satisfies differential privacy.

As we will be perturbing this utility measure we are interested in a sensitivity measure for it; since we are only interested in its sensitivity relative to the data set argument, we define this as the smallest number $\Delta u$ such that for any $r \in \fancy{R}$, and any two data sets $D_1,D_2 \in \fancy{D}$ differing in exactly one entry,
\begin{equation}\label{eq:utility_sensitivity}
    |u(D_1,r) - u(D_2,r)| \leq \Delta u.
\end{equation}

From here we define the actual mechanism to be
\begin{equation*}
    f'(d) = \text{ Return } r \in \fancy{R} \text{ with probability} \propto \exp\left( \frac{\epsilon u(d,r)}{2\Delta u} \right).
\end{equation*}
To show that this is differentially private we will once again look at two data sets $D_1,D_2$ differing in at most one entry, and some fixed output $r \in \fancy{R}$. If we name the proportional component $\lambda(d,r) = \exp \left( \epsilon u(d,r) / 2 \Delta u \right)$ we then have
\begin{align}
    \frac{\Pr[f'(D_1) = r]}{\Pr[f'(D_2) = r]} &= 
                \left( \frac{\lambda(D_1,r)}{\sum_{r' \in \fancy{R}} \lambda(D_1,r')} \right)
                /
                \left( \frac{\lambda(D_2,r)}{\sum_{r' \in \fancy{R}} \lambda(D_2,r')} \right) \nonumber\\
            &= \left( \frac{\lambda(D_1,r)}{\lambda(D_2,r)} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \nonumber\\
            &= \left( \frac{\exp(\epsilon u(D_1,r)/2\Delta u)}{\exp(\epsilon u(D_2,r)/2\Delta u)} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \nonumber\\
            &= \exp \left( \frac{\epsilon u(D_1,r)}{2\Delta u} - \frac{\epsilon u(D_2,r)}{2\Delta u} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \nonumber\\
            &= \exp \left( \frac{\epsilon (u(D_1,r) - u(D_2,r))}{2\Delta u}\right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \nonumber\\
            &\leq \exp \left( \frac{\epsilon \Delta u}{2\Delta u}\right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \nonumber\\
            &= \exp \left( \frac{\epsilon}{2} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \label{eq:expmech}
\end{align}
where the inequality comes from definition \ref{eq:utility_sensitivity}. From the same definition we have that
\begin{align*}
    u(D_2,r) &= u(D_1,r) + u(D_2,r) - u(D_1,r) \\
        &\leq u(D_1,r) + \Delta u
\end{align*}
$$\Downarrow$$
\begin{align*}
    \lambda(D_2,r) &= \exp\left( \frac{\epsilon u(D_2,r)}{2\Delta u} \right) \\
        &\leq \exp\left( \frac{\epsilon u(D_1,r) + \epsilon\Delta u}{2\Delta u} \right) \\
        &= \exp\left( \frac{\epsilon u(D_1,r)}{2\Delta u} + \frac{\epsilon}{2} \right) \\
        &= \lambda(D_1,r) \cdot \exp \left( \frac{\epsilon}{2} \right)
\end{align*}
which can finally be used to rewrite equation \ref{eq:expmech} into
\begin{align*}
    \frac{\Pr[f'(D_1) = r]}{\Pr[f'(D_2) = r]} &\leq \exp \left( \frac{\epsilon}{2} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_2,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \\
        &\leq \exp\left( \frac{\epsilon}{2} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_1,r')\cdot \exp(\epsilon/2)}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \\
        &= \exp\left( \frac{\epsilon}{2} \right) \cdot \exp\left( \frac{\epsilon}{2} \right) \cdot \left( \frac{\sum_{r'\in\fancy{R}} \lambda(D_1,r')}{\sum_{r'\in\fancy{R}} \lambda(D_1,r')} \right) \\
        &= \exp(\epsilon).
\end{align*}
This proves that $f'$ is $\epsilon$-differentially private. \bigskip

This mechanism offers some attributes that the Laplace mechanism does not: for one, it can be used for algorithms with arbitrary ranges, not just numeric ones. This makes it more generally applicable than the Laplace mechanism (in fact it covers every application of the Laplace mechanism with the utility function $u(d,r)=-|f(d)-r|$ \cite{sherry_exponentialmech}).

In addition, the actual output of the algorithm is not perturbed, only the probability of picking the most ``ideal'' output is. This means that it is well suited for applications where the precise value of the output is important, e.g. auctions, where even small changes to the final price are unacceptable.

As with any differentially private algorithm, it is of course important to note that there are no outputs in the range of $f'$ with probability 0, regardless of input---that is, there is some probability that $f'(d)$ will pick a ``bad'' output $r$ regardless of how low $u(d, r)$ is. As the value of $u(d,r)$ is used as an exponent in the proportional part $\lambda(d,r)$, this probability does however decrease quickly as the utility score lowers.

\subsection{Models and variants of differential privacy \label{sec:variants}}

The definition of $\epsilon$-differential privacy puts no restrictions on the specific way a private algorithm is implemented beyond the probability of each output. This makes it possible to implement private algorithms in multiple different ways, with different implications for each. We have already seen in the Laplace and exponential mechanisms examples of achieving privacy through perturbation of the output. In this section we will explore the case of \emph{local differential privacy}, which is an alternative approach that removes the need for subjects to trust the data collector with their true data. We will also explore a slightly extended variant of $\epsilon$-differential privacy, known as $(\epsilon,\delta)$-differential privacy.

\subsubsection{Local differential privacy}

The local model for differential privacy \cite{kasiv_localdiff} is an extension of the idea behind randomized response as introduced in section \ref{sec:motivation} (and is sometimes named after it). It describes a subset of $\epsilon$-differentially private algorithm for which each subject randomizes their own private data before entering it into the data set. This means there is no central data set of private data from which the output is computed; instead, the perturbation happens before the data is entered into the data set.

Formally an algorithm is $\epsilon$-local if it works on a data set $D$ where each entry $x_i \in D$ is computed by an $\epsilon_i$-differentially private algorithm such that $\sum_{i\in\left[|D|\right]} \epsilon_i \leq \epsilon$. From the compositions theorems we showed in section \ref{sec:composition} it is clear that any $\epsilon$-local algorithm is also $\epsilon$-differentially private.

The local model represents a significant shift in the trust model for private algorithms. In the global model, where a data collector collects a data set of private data before perturbing it, each subject must reveal their private data to at least one entity, the data collector. In the local model however, this is no longer necessary. Each subject can be absolutely certain that their private data cannot be revealed unless they choose to release it. This is especially important in situations where the data collector cannot be trusted, or has an incentive to violate subject privacy; for this reason it is the local model that has been employed in many of the private telemetry algorithms for use by e.g. Google \cite{google_rappor}, Apple \cite{apple_differential} and Microsoft \cite{microsoft_telemetry}.

It is worth noting however that the local model is strictly weaker than the global model. In addition to worse lower bounds on the error for the perturbed output, it also isn't every query that can be released under the global model that can be released under the local model \cite[sec.~12.1]{dwork_privacybook}.

\subsubsection{$(\epsilon, \delta)$-differential privacy \label{sec:variant_eps_delta}}

The privacy guarantees of $\epsilon$-differential privacy are strong. This is desirable as it means that any $\epsilon$-private algorithm is sure to act in a specific, well understood way. However, it also means that algorithms that fail to achieve $\epsilon$-differential privacy range from almost-private algorithms to those that offer no privacy whatsoever. For some use cases it may be useful to offer weaker guarantees, as these allow us to better describe algorithms that may not achieve $\epsilon$-differential privacy, but could achieve some other notion of privacy.

Of particular interest is a variant of $\epsilon$-differential privacy known as $(\epsilon, \delta)$-differential privacy \cite{dwork2006_delta_diffpriv}. It alters definition \ref{eq:diffpriv} slightly such that for any two neighboring data sets $D_1$ and $D_2$,
\begin{equation*}
    \Pr[\fancy{K}(D_1) \in S] \leq \exp(\epsilon) \cdot \Pr[\fancy{K}(D_2) \in S] + \delta.
\end{equation*}

This additional $\delta$ term allows private algorithms to ``fail'' to uphold the $\epsilon$-differential privacy definition of privacy sometimes. It is especially impactful for rare events, where the probabilities are low, and $\delta$ is comparably high.

This ``failure'' is worrisome from a privacy perspective. In particular, it allows events to have a probability of 0 unless a specific element is in the data set, in which case it has a probability of at most $\delta$. If such an event is seen, it causes a total privacy breach for the entry in question. This is naturally undesirable. As such, low $\delta$ values are desired for $(\epsilon,\delta)$-private algorithms.

It is worth noting that, unlike the $\epsilon$ parameter, the desirable values for $\delta$ depend on the population size in question. The likelihood of a ``failure'' is at most $\delta$ for each entry in the data set. As this is given independently for each entry, the expected probability of a ``failure'' for any one of $n$ entries in a data set is $\delta n$.\bigskip

The addition of the $\delta$ parameter allows considerably more freedom in terms of algorithm design. As $(\epsilon,\delta)$-differential privacy is a superset of $\epsilon$-differential privacy, every theorem that we have shown so far also holds for this new definition---but due to the relaxed privacy bounds, new theorems are applicable that are not applicable to our old definition. Of particular note is more advanced composition theorems \cite[Sec.~3.5.2]{dwork_privacybook}, which describe many of the compositions that may happen during real world usage.

\newpage
\section{Experiments with low-population telemetry collection \label{sec:telemetry}}

In the following section we will be focusing on a 2017 paper \cite{microsoft_telemetry} describing a differentially private algorithm in the local model for mean and histogram estimation.

The paper builds on previous research \cite{google_rappor} into private algorithms that support repeated collection of the same data from subjects. This is important for common applications of private algorithms like telemetry collection. Historically this has been problematic, as the privacy guarantees degrade quickly as repeated collections are made over time. As a simple example, consider a private algorithm implemented by adding noise to the true value during each collection: as more and more runs are collected, an adversary will become more and more certain of the true value at the center of each of the observed responses.

Previous research \cite{google_rappor} proposes using memoization to solve this issue: by remembering the perturbed value given as a response for a given true value, the subject can simply respond with the same perturbed value during the next collection. This ensures that the privacy guarantees made for a single collection hold even when collecting repeatedly---as long as the underlying true value does not change. This approach is weakened when the true value is expected to change frequently, even if the changes are small. Although this can be partially alleviated by discretizing the true values prior to memoization, the problem persists for values near the edges of the discretization buckets.

\subsection{The privacy framework \label{sec:framework}}

The paper \cite{microsoft_telemetry} introduces a three part framework for designing algorithms that offer strong privacy guarantees over repeated collections. The goal of the framework is to allow repeated private collections even in the case where the true value changes between collections. It consists of three steps: first the true value is discretized using an $\alpha$-point rounding scheme, then the discretized value is privatized and memoized. Finally the output is perturbed to mask transitions across discretization buckets.

\subsubsection{$\alpha$-point rounding \label{sec:alpha_rounding}}

The rounding step is an extension of the discretization step from previous approaches. The goal is to mask small changes to the true value by rounding it into ``buckets'', such that values that are close to each other map to the same memoized value. This helps avoid the problem of small changes in the true value leading to large privacy losses, as close values tend to be treated identically.

If implemented naively this may however have a detrimental effect on the accuracy of the algorithm. Since the true value is rounded to a fixed point in its respective bucket, the discretized value may be some distance away from the true value, depending on bucket size. If the true values tend to be correlated across subjects, e.g. if every subject has roughly the same true value, then this rounding may lead to significant inaccuracy in the algorithm as each subject reports values rounded in the same direction.

The idea of $\alpha$-point rounding is to avoid this issue by choosing a rounding direction based on an $\alpha$ value picked independently by each subject. In this way the values after discretization will, when seen across many subjects, roughly center on their true values---even in the case where each subject has the same true value. \bigskip

Formally the $\alpha$-point rounding algorithm is defined as follows:

First, the algorithm designer decides on a bucket size $s$. It is assumed that the true values we wish to discretize are in the range $[0,m]$, and that the algorithm designer picks $s$ such that it divides $m$. This means that we have a total of $m/s$ different values we can memoize to. The paper suggests picking a relatively large $s$ compared to $m$, presumably to protect subject privacy---as $s$ approaches 0, the discretized output approaches the true value, invalidating the rounding step.

As each subject is initialized, before any collection takes place, each subject picks a value $\alpha_i \in [0,s-1]$ uniformly at random. This value remains unchanged throughout the lifetime of the subject.

When collection is requested for a given subject, its true value $x_i$ is mapped to its two neighboring bucket edges $L=s \lfloor x_i / s \rfloor$ and $R=L+s$. The discrete output of the rounding algorithm is then chosen based on the $\alpha_i$ value of the subject: if $x_i + \alpha_i < R$ then the discrete output is $L$. Otherwise it is $R$. \bigskip

Beyond alleviating the accuracy problems of conventional rounding techniques, this approach also helps avoid a class of privacy leaks based on the discretized value.

Consider the example where the true value each subject reports is their age in days. If this value was collected by first discretizing it using a conventional rounding algorithm (where $x_i$ is mapped to some fixed value for its corresponding bucket), and then memoizing the discrete value after privatization, an adversary may still be able to learn the true value given enough collections. Specifically, if the adversary observes a change in reported value from a subject, it knows that said subject has transitioned from one bucket to another. Depending on how the value is privatized it may be able to determine which bucket was transitioned into, and therefore learn the true value of the subject.

This is not possible using $\alpha$-point rounding. Although the adversary may still learn the time at which the reported value transitions from one bucket to the next, this is safe from a privacy perspective as this transition point is defined by $\alpha_i$. Since $\alpha_i$ is picked randomly by each subject, the adversary cannot use it to learn the true value of the subject---only the bucket to which it belongs.

\subsubsection{Privatization and memoization}

After discretization, the value is privatized using some differentially private algorithm. The output of this algorithm is memoized, such that future collections of the same discrete value will give the same output. This is crucial, as explained earlier, in order to avoid degradation of the privacy guarantees under repeated collection. This memoization is permanent, meaning a mapping of discrete value to output exists for the lifetime of the subject once created.

\subsubsection{Output perturbation}

As explained in section \ref{sec:alpha_rounding}, even with $\alpha$-point rounding, an adversary may still learn the point in time at which the subjects reported value transitions from one discretized value to another. The goal of output perturbation is to mask this transition.

Formally, the output perturbation consists of flipping each bit in the memoized output with some small probability $\gamma$ before responding. If $b_i(t)$ is the $i$th bit of given subjects discrete value for a given time $t$, then the perturbed bit $\hat{b}_i(t)$ is defined as $$\hat{b}_i(t)=\begin{cases}
b_i(t) & \text{with probability 1 } - \gamma, \\
1 - b_i(t) & \text{otherwise.}
\end{cases}$$

This random perturbation ensures that an adversary observing the responses for a given subject cannot learn the \emph{exact} point in time at which the discrete value changed. Rather, it can be shown \cite[sec.~4]{microsoft_telemetry} that there is a window in time centered around the true time $t$, during which the adversary cannot differentiate if the change happened at time $t$, or at any other time $t'$ in the window. This window is specifically $[t-d, t+d]$, where $d$ is the Hamming distance between the two memoized values.

\subsection{Histogram estimation}

The paper introduces two $\epsilon$-differentially private algorithms in the local setting, one for mean estimation, and one for histogram estimation. As the experiments in section \ref{sec:experiments} will focus on the histogram estimation algorithm, we will not explore the mean estimation algorithm. Instead, the following section will explain the basics of the histogram estimation algorithm. \bigskip

Histogram estimation is the problem of collecting from each subject the bucket into which their true value falls, and estimating the histogram across the total population of subjects.

In the algorithm described in \cite{microsoft_telemetry}, called $d\textsc{BitFlip}$, the histogram is defined as $k$ buckets. The goal of the algorithm is to estimate how many subjects fall into each of the $k$ bucket, given their differentially private responses.

At initialization time, before any collection takes place, each subject draws $d$ bucket numbers without replacement from $[k]$, denoted $j_1,j_2,\dots,j_d$. These bucket numbers are stored permanently, and re-used for each collection.

During collection, each user first maps their true value $x_i(t)$ to its discrete bucket number $v_i(t)$. They then respond with a vector $$b_i(t)=\left[ (j_1, b_{i,j_1}(t)), (j_2, b_{i,j_2}(t)), \dots, (j_d, b_{i,j_d}(t)) \right]$$
consisting of their chosen bucket numbers, each with an accompanying random bit $b_{i,j_p}(t)$. This random bit has probability
$$\Pr[b_{i,j_p}(t)=1]=\begin{cases}
    \frac{\exp(\epsilon/2)}{\exp(\epsilon / 2) + 1} & \text{if } v_i(t) = j_p, \\
    \frac{1}{\exp(\epsilon/2) + 1} & \text{otherwise.}
\end{cases}$$

Given a set of $n$ such response vectors, the data collector is able to estimate the histogram $h_t$ for each bucket number $v \in [k]$ using the formula
\begin{equation*}
    \hat{h}_t(v) = \frac{k}{nd} \sum_{\text{received } b_{i,v}(t)} \frac{b_{i,v}(t)\cdot (\exp(\epsilon/2) + 1) - 1}{\exp(\epsilon/2) - 1}.
\end{equation*}
The accuracy of the estimated value for each bucket depends on the values of $k$, $\epsilon$, $n$ and $d$ as shown in \cite[sec.~2.2]{microsoft_telemetry}. Specifically, with probability at least $1 - \delta$ we have
$$
\max_{v \in [k]} |h_t(v) - \hat{h}_t(v)| \leq O\left( \sqrt{\frac{k \log(k/\delta)}{\epsilon^2nd}} \right).
$$ \bigskip

This histogram estimation algorithm can be extended using the framework described in section \ref{sec:framework}. This extension of $d\textsc{BitFlip}$, called $d\textsc{BitFlipPM}$, is the algorithm we will use for experimentation in section \ref{sec:experiments}.

\subsection{Low population experiments \label{sec:experiments}}

The algorithms have been subject to empirical evaluation \cite[sec.~5]{microsoft_telemetry}, testing their estimation error at various $d$ and $\epsilon$ parameters, but this has only been done for population sizes of at least $0.3\times 10^6$. This leaves many applications---such as telemetry collection by companies smaller than Microsoft---with little information on the applicability of the mechanisms.

In this section we will extend the experiments for the histogram estimation algorithm to lower population sizes. We expect, due to the nature of differential privacy, that the estimation errors will increase significantly as population size decreases, especially at lower $\epsilon$ parameters. Nonetheless, the exact nature of this relationship is worth exploring. \bigskip

Our implementation of the algorithms, along with the code for these experiments, can be found at \cite{git_repo}.

Similar to the original experiments, we will be using three synthetic data sets sampled from constant, uniform and (truncated) normal distributions, respectively. The exact distributions are identical to those from \cite{microsoft_telemetry}, although the data sets may naturally vary. In order to keep our results comparable, we use the similar parameters to those in \cite{microsoft_telemetry}, beyond population size: we use $k=32$ evenly sized buckets on the range $[0,24]$, and permanent memoization for repeated collection but no output perturbation. We vary population size in the range $[10^3, 0.3 \times 10^6]$, and $\epsilon$ in the range $[0.1, 10]$.

For each data set we measure the \emph{histogram error}, defined as the maximum error across all buckets of the histogram:
\begin{equation*}
    \text{Error}\left(h(\cdot), \hat{h}(\cdot)\right) = \max_{v\in [k]} |h(v) - \hat{h}(v)|
\end{equation*}

Although this measure is useful for finding the estimation error for individual buckets, it does not accurately measure the estimation accuracy of the distribution itself. For this purpose we will instead use a measure known as the Wasserstein distance. Intuitively this measures the amount of ``work'' required to transform one distribution into another, where work is defined as the amount of distribution weight that must be moved, multiplied by the distance it must be moved (this distance measure is also known as the earth mover's distance). Formally it can be calculated using the CDFs $U$ and $V$ of the two distributions to be compared, and is defined as
\begin{equation*}
    \text{Wasserstein}(U,V) = \int_{-\infty}^\infty |U-V|.
\end{equation*}

In order to minimize the impact of the random nature of the algorithm, we calculate the metrics by averaging over 10 separate executions. \bigskip

For the sake of gaining an intuitive understanding of the problem, a visual representation of the histogram estimation for our normal data set can be seen in figure \ref{fig:histogram_matrix}. This supports our expectation that low population sizes cause worse histogram estimations, especially at low $\epsilon$ parameters.

Notably it appears that at low enough $\epsilon$ parameters, the histogram estimation does not significantly improve before we reach the highest population size $n=0.3 \times 10^6$ of our experiments.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imgs/histogram_matrix.png}
    \caption{Examples of estimated histograms for various $\epsilon$ and population sizes with $d=1$.}
    \label{fig:histogram_matrix}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \subcaptionbox{Normal distribution}
            {\includegraphics[width=0.6\textwidth]{imgs/histogram_err_normal.png}
            \includegraphics[width=0.6\textwidth]{imgs/histogram_wassersteins_normal.png}}
    }
    \makebox[\textwidth][c]{
        \subcaptionbox{Uniform distribution}
            {\includegraphics[width=0.6\textwidth]{imgs/histogram_err_uniform.png}
            \includegraphics[width=0.6\textwidth]{imgs/histogram_wassersteins_uniform.png}}
    }
    \makebox[\textwidth][c]{
        \subcaptionbox{Constant distribution}
            {\includegraphics[width=0.6\textwidth]{imgs/histogram_err_constant.png}
            \includegraphics[width=0.6\textwidth]{imgs/histogram_wassersteins_constant.png}}
    }
    \caption{Estimation errors for the histogram estimation mechanism at various $\epsilon$ and population sizes. Solid, dashed and dotted lines are $d=1,2,4$ respectively.}
    \label{fig:histogram_errors}
\end{figure}

\subsubsection{Results}

The results of our experiments can be seen in figure \ref{fig:histogram_errors}. As expected, low population sizes give significantly worse estimation errors: at $n=10^4$ and $\epsilon=1,d=1$ the average estimation error for our normal data set is 0.3, compared to just 0.045 when $n=3\times10^5$. In other words, the bucket with the largest misestimation was off by 30\% at $n=10^4$. At $n=3\times 10^5$ no bucket was misestimated by more than 4.5\%. The histogram error appears to be exponentially correlated to population size. These results agree with those found in \cite{microsoft_telemetry}.

The Wasserstein distance further supports these results. Unlike the histogram error, the Wasserstein distance supports the intuitive understanding of the distribution estimations gained from figure \ref{fig:histogram_matrix}. Specifically, the distribution estimation does not appear to improve notably for $\epsilon=0.1$ until population sizes beyond our scope. For each of the other $\epsilon$ parameters the estimation improves as population size increases. We expect that even $\epsilon=0.1$ will slowly improve as population sizes increase beyond those tested here. \bigskip

These results are promising even for relatively small population sizes. Depending on whether the utility of the histogram estimation for a particular use case lies in distribution estimation or individual bucket estimation, the specific error measure varies.

For the histogram error, measuring individual bucket estimation error, the error quickly decreases as population size goes up. This effect is particularly pronounced for low $\epsilon$ parameters, where the relative decrease as population size increases is greatest.

For the Wasserstein distance, measuring distribution estimation error, the error also decreases as population size goes up. This effect is particularly pronounced for medium $\epsilon$ parameters; for low $\epsilon$ parameters the measure does not significantly increase during the tested population sizes, and for high $\epsilon$ parameters the measure is low even for low population sizes. This leaves the in-between $\epsilon$ parameters for which the measure improves as expected, decreasing as population size and $\epsilon$ increases.

\section{Conclusions}

In this thesis we have explored the topic of differential privacy in chapter \ref{sec:diffpriv}. We have covered its definition and the meaning of the $\epsilon$ parameter. We have covered composition theorems describing the characteristics of combined differentially private algorithms, along with methods for achieving differential privacy for a group of common statistical problems. Finally we have covered common variants of differential privacy, extending the notion to other application areas.

In chapter \ref{sec:telemetry} we have further explored one specific differentially private algorithm. We have experimentally determined its characteristics for small population sizes, an area which was not covered in the original paper. We have applied a formal measure that better describes the distribution estimation capabilities than that used in the paper, and used it to describe the quality of the estimation as $\epsilon$ and population size varies. In particular, we have found that for practical values of $\epsilon$ (e.g. $\epsilon = 3$) the distributions improve significantly even for low population sizes, with the Wesserstein distance nearing 2 at a population size of $10^4$, reaching 1 at $n=10^5$. These results are promising for applications of the algorithm in analytical applications that do not have the user base of Windows 10, which the algorithm was developed for.

\bigskip

Future work may further explore differentially private algorithms; this is a rapidly evolving field, with ample opportunity for further research or literature review. In particular, the application of the memoization techniques from \cite{microsoft_telemetry} on other algorithms in order to extend their privacy guarantees to temporally repeated collection could be an area of interest, as this is a feature often used in telemetry collection.

\newpage

\printbibliography

\end{document}
