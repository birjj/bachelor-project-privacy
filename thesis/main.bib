@inproceedings{revealing_information,
  author    = {Dinur, Irit and Nissim, Kobbi},
  title     = {Revealing Information While Preserving Privacy},
  year      = {2003},
  isbn      = {1581136706},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/773153.773173},
  doi       = {10.1145/773153.773173},
  abstract  = {We examine the tradeoff between privacy and usability of statistical databases. We model a statistical database by an n-bit string d1,..,dn, with a query being a subset q ⊆ [n] to be answered by Σiεq di. Our main result is a polynomial reconstruction algorithm of data from noisy (perturbed) subset sums. Applying this reconstruction algorithm to statistical databases we show that in order to achieve privacy one has to add perturbation of magnitude (Ω√n). That is, smaller perturbation always results in a strong violation of privacy. We show that this result is tight by exemplifying access algorithms for statistical databases that preserve privacy while adding perturbation of magnitude \~{O}(√n).For time-T bounded adversaries we demonstrate a privacypreserving access algorithm whose perturbation magnitude is ≈ √T.},
  booktitle = {Proceedings of the Twenty-Second ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  pages     = {202–210},
  numpages  = {9},
  keywords  = {subset-sums with noise, data reconstruction, integrity and security},
  location  = {San Diego, California},
  series    = {PODS '03}
}

@article{dalenius1977,
  title   = {Towards a methodology for statistical disclosure control},
  author  = {Dalenius, Tore},
  journal = {statistik Tidskrift},
  volume  = {15},
  number  = {429-444},
  pages   = {2--1},
  year    = {1977}
}

@incollection{dwork2006_diffpriv,
  doi       = {10.1007/11787006_1},
  url       = {https://doi.org/10.1007/11787006_1},
  year      = {2006},
  publisher = {Springer Berlin Heidelberg},
  pages     = {1--12},
  author    = {Cynthia Dwork},
  title     = {Differential Privacy},
  booktitle = {Automata,  Languages and Programming}
}

@incollection{dworketal2006,
  doi       = {10.1007/11681878_14},
  url       = {https://doi.org/10.1007/11681878_14},
  year      = {2006},
  publisher = {Springer Berlin Heidelberg},
  pages     = {265--284},
  author    = {Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam Smith},
  title     = {Calibrating Noise to Sensitivity in Private Data Analysis},
  booktitle = {Theory of Cryptography}
}

@article{reidentification2011,
  author  = {Emam, Khaled and Jonker, Elizabeth and Arbuckle, Luk and Malin, Bradley},
  year    = {2011},
  month   = {12},
  pages   = {e28071},
  title   = {A Systematic Review of Re-Identification Attacks on Health Data},
  volume  = {6},
  journal = {PloS one},
  doi     = {10.1371/journal.pone.0028071}
}

@incollection{dwork2006_delta_diffpriv,
  doi       = {10.1007/11761679_29},
  url       = {https://doi.org/10.1007/11761679_29},
  year      = {2006},
  publisher = {Springer Berlin Heidelberg},
  pages     = {486--503},
  author    = {Cynthia Dwork and Krishnaram Kenthapadi and Frank McSherry and Ilya Mironov and Moni Naor},
  title     = {Our Data,  Ourselves: Privacy Via Distributed Noise Generation},
  booktitle = {Advances in Cryptology - {EUROCRYPT} 2006}
}

@inproceedings{microsoft_telemetry,
  author    = {Ding, Bolin and Kulkarni, Janardhan and Yekhanin, Sergey},
  title     = {Collecting Telemetry Data Privately},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {3574–3583},
  numpages  = {10},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{precursor_2003,
  author    = {Dinur, Irit and Nissim, Kobbi},
  title     = {Revealing Information While Preserving Privacy},
  year      = {2003},
  isbn      = {1581136706},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/773153.773173},
  doi       = {10.1145/773153.773173},
  abstract  = {We examine the tradeoff between privacy and usability of statistical databases. We model a statistical database by an n-bit string d1,..,dn, with a query being a subset q ⊆ [n] to be answered by Σiεq di. Our main result is a polynomial reconstruction algorithm of data from noisy (perturbed) subset sums. Applying this reconstruction algorithm to statistical databases we show that in order to achieve privacy one has to add perturbation of magnitude (Ω√n). That is, smaller perturbation always results in a strong violation of privacy. We show that this result is tight by exemplifying access algorithms for statistical databases that preserve privacy while adding perturbation of magnitude \~{O}(√n).For time-T bounded adversaries we demonstrate a privacypreserving access algorithm whose perturbation magnitude is ≈ √T.},
  booktitle = {Proceedings of the Twenty-Second ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  pages     = {202–210},
  numpages  = {9},
  keywords  = {data reconstruction, subset-sums with noise, integrity and security},
  location  = {San Diego, California},
  series    = {PODS '03}
}

@incollection{precursor_2004,
  doi       = {10.1007/978-3-540-28628-8_32},
  url       = {https://doi.org/10.1007/978-3-540-28628-8_32},
  year      = {2004},
  publisher = {Springer Berlin Heidelberg},
  pages     = {528--544},
  author    = {Cynthia Dwork and Kobbi Nissim},
  title     = {Privacy-Preserving Datamining on Vertically Partitioned Databases},
  booktitle = {Advances in Cryptology {\textendash} {CRYPTO} 2004}
}

@inproceedings{precusor_2005,
  doi       = {10.1145/1065167.1065184},
  url       = {https://doi.org/10.1145/1065167.1065184},
  year      = {2005},
  publisher = {{ACM} Press},
  author    = {Avrim Blum and Cynthia Dwork and Frank McSherry and Kobbi Nissim},
  title     = {Practical privacy},
  booktitle = {Proceedings of the twenty-fourth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on Principles of database systems  - {PODS} {\textquotesingle}05}
}

@misc{apple_differential,
  title   = {Differential {Privacy}},
  url     = {https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf},
  urldate = {2021-05-14},
  note    = {publisher: Apple}
}

@article{apple_differential_loss,
  author = {Tang, Jun and Korolova, Aleksandra and Bai, Xiaolong and Wang, Xueqiang and Wang, Xiaofeng},
  year   = {2017},
  month  = {09},
  pages  = {},
  title  = {Privacy Loss in Apple's Implementation of Differential Privacy on MacOS 10.12}
}

@inproceedings{google_rappor,
  author    = {Erlingsson, \'{U}lfar and Pihur, Vasyl and Korolova, Aleksandra},
  title     = {RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response},
  year      = {2014},
  isbn      = {9781450329576},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2660267.2660348},
  doi       = {10.1145/2660267.2660348},
  abstract  = {Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.},
  booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {1054–1067},
  numpages  = {14},
  keywords  = {cloud computing, population statistics, privacy protection, crowdsourcing, statistical inference},
  location  = {Scottsdale, Arizona, USA},
  series    = {CCS '14}
}

@inproceedings{google_prochlo,
  author    = {Bittau, Andrea and Erlingsson, \'{U}lfar and Maniatis, Petros and Mironov, Ilya and Raghunathan, Ananth and Lie, David and Rudominer, Mitch and Kode, Ushasree and Tinnes, Julien and Seefeld, Bernhard},
  title     = {Prochlo: Strong Privacy for Analytics in the Crowd},
  year      = {2017},
  isbn      = {9781450350853},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132747.3132769},
  doi       = {10.1145/3132747.3132769},
  abstract  = {The large-scale monitoring of computer users' software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring.With ESA, the privacy of monitored users' data is guaranteed by its processing in a three-step pipeline. First, the data is encoded to control scope, granularity, and randomness. Second, the encoded data is collected in batches subject to a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get "lost in the crowd" of the batch. Third, the anonymous, shuffled data is analyzed by a specific analysis engine that further prevents statistical inference attacks on analysis results.ESA extends existing best-practice methods for sensitive-data analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis.Strong privacy guarantees, including differential privacy, can be established at each processing step to defend against malice or compromise at one or more of those steps. Prochlo develops new techniques to harden those steps, including the Stash Shuffle, a novel scalable and efficient oblivious-shuffling algorithm based on Intel's SGX, and new applications of cryptographic secret sharing and blinding. We describe ESA and Prochlo, as well as experiments that validate their ability to balance utility and privacy.},
  booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
  pages     = {441–459},
  numpages  = {19},
  location  = {Shanghai, China},
  series    = {SOSP '17}
}


@inproceedings{ml_abadi,
  address   = {Vienna Austria},
  title     = {Deep {Learning} with {Differential} {Privacy}},
  isbn      = {9781450341394},
  url       = {https://dl.acm.org/doi/10.1145/2976749.2978318},
  doi       = {10.1145/2976749.2978318},
  language  = {en},
  urldate   = {2021-05-16},
  booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
  publisher = {ACM},
  author    = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  month     = oct,
  year      = {2016},
  pages     = {308--318}
}

@inproceedings{ml_shokri,
  address   = {Monticello, IL},
  title     = {Privacy-preserving deep learning},
  isbn      = {9781509018246},
  url       = {http://ieeexplore.ieee.org/document/7447103/},
  doi       = {10.1109/ALLERTON.2015.7447103},
  urldate   = {2021-05-16},
  booktitle = {2015 53rd {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
  publisher = {IEEE},
  author    = {Shokri, Reza and Shmatikov, Vitaly},
  month     = sep,
  year      = {2015},
  pages     = {909--910}
}

@article{ml_papernot,
  title    = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
  url      = {https://arxiv.org/abs/1610.05755v4},
  abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
  language = {en},
  urldate  = {2021-05-16},
  author   = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
  month    = oct,
  year     = {2016}
}

@misc{us_census,
  title   = {2020 {Census} {Data} {Products}: {Disclosure} {Avoidance} {Modernization}},
  url     = {https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance.html},
  urldate = {2021-05-16},
  journal = {census.gov}
}
